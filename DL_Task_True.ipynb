{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MKd_7BeVBP0"
      },
      "source": [
        "# **DATA LOADING**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (Dense, Conv1D, MaxPooling1D, LSTM, Bidirectional,\n",
        "                                     Embedding, Dropout, SpatialDropout1D, GlobalMaxPooling1D)\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "data = pd.read_csv('/content/customer_service_sentiment - meaningful_customer_service_sentiment.csv.csv')\n"
      ],
      "metadata": {
        "id": "Hf8qcNhYQb6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATA PROCESSING**"
      ],
      "metadata": {
        "id": "bNeUpsooEIak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\W', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# preprocessing\n",
        "data['processed_text'] = data['customer_message'].apply(preprocess_text)\n",
        "\n",
        "#MAPPING sentiments\n",
        "sentiment_mapping = {'Frustrated': 0, 'Neutral': 1, 'Satisfied': 2}\n",
        "data['sentiment_label'] = data['sentiment_label'].map(sentiment_mapping)\n"
      ],
      "metadata": {
        "id": "tw3121VGD-p-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TEXT EMBEDDINGS**"
      ],
      "metadata": {
        "id": "fFWpCKK3ENgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Bag of Words (BoW)\n",
        "bow_vectorizer = CountVectorizer(max_features=5000)\n",
        "X_train_bow = bow_vectorizer.fit_transform(train_data['processed_text']).toarray()\n",
        "X_val_bow = bow_vectorizer.transform(val_data['processed_text']).toarray()\n",
        "X_test_bow = bow_vectorizer.transform(test_data['processed_text']).toarray()\n",
        "\n",
        "# TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(train_data['processed_text']).toarray()\n",
        "X_val_tfidf = tfidf_vectorizer.transform(val_data['processed_text']).toarray()\n",
        "X_test_tfidf = tfidf_vectorizer.transform(test_data['processed_text']).toarray()\n",
        "\n",
        "print(\"Embedding completed for BoW and TF-IDF.\")\n"
      ],
      "metadata": {
        "id": "NEqVbfiwETnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MODEL TRAINING WITH DIFFERENT ARCHITECTURES**"
      ],
      "metadata": {
        "id": "DAcbDsxVEVZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **COMPARATIVE ANALYSIS OF MODELS**"
      ],
      "metadata": {
        "id": "-5zgn1FqExQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cnn_model.save('best_cnn_model.h5')\n",
        "print(\"Best CNN model saved to best_cnn_model.h5\")\n"
      ],
      "metadata": {
        "id": "QaB-yS4aE8V9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MODEL TRAINING AND COMAPRATIVE ANALYSIS**"
      ],
      "metadata": {
        "id": "Ek7hF3sZE96-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dropout_rate = 0.65\n",
        "learning_rate = 0.0002\n",
        "l2_factor = 1e-4\n",
        "batch_size = 32\n",
        "\n",
        "# ==============================\n",
        "# CNN Model\n",
        "# ==============================\n",
        "cnn_model = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=128),\n",
        "    SpatialDropout1D(0.5),  # Slightly reduced dropout on embeddings\n",
        "    Conv1D(filters=32, kernel_size=5, activation='relu', kernel_regularizer=l2(l2_factor)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dropout(dropout_rate),\n",
        "    Dense(16, activation='relu', kernel_regularizer=l2(l2_factor)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    Dropout(dropout_rate),\n",
        "    Dense(3, activation='softmax')\n",
        "])\n",
        "cnn_model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate), metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# LSTM Model\n",
        "# ==============================\n",
        "lstm_model = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=128),\n",
        "    SpatialDropout1D(0.8),\n",
        "    LSTM(32, return_sequences=True, dropout=dropout_rate, recurrent_dropout=0.2, kernel_regularizer=l2(1e-3)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    LSTM(16, dropout=dropout_rate, kernel_regularizer=l2(1e-3)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    Dense(16, activation='relu', kernel_regularizer=l2(1e-3)),\n",
        "    Dropout(dropout_rate),\n",
        "    Dense(3, activation='softmax')\n",
        "])\n",
        "lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate), metrics=['accuracy'])\n",
        "\n",
        "# ==============================\n",
        "# Revised BiLSTM Model\n",
        "# ==============================\n",
        "bilstm_model = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=128),\n",
        "    SpatialDropout1D(0.8),\n",
        "    Bidirectional(LSTM(32, return_sequences=True, dropout=dropout_rate, recurrent_dropout=0.2, kernel_regularizer=l2(1e-3))),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    Bidirectional(LSTM(16, dropout=dropout_rate, kernel_regularizer=l2(1e-3))),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    Dense(16, activation='relu', kernel_regularizer=l2(1e-3)),\n",
        "    Dropout(dropout_rate),\n",
        "    Dense(3, activation='softmax')\n",
        "])\n",
        "bilstm_model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate), metrics=['accuracy'])\n",
        "\n",
        "# ==========================\n",
        "# Train Models with individual epoch values\n",
        "# ==========================\n",
        "print(\"Training CNN Model...\")\n",
        "history_cnn = cnn_model.fit(X_train, y_train,\n",
        "                            validation_data=(X_val, y_val),\n",
        "                            epochs=epochs_cnn,\n",
        "                            batch_size=batch_size,\n",
        "                            callbacks=[early_stop, reduce_lr],\n",
        "                            verbose=1)\n",
        "\n",
        "print(\"Training LSTM Model...\")\n",
        "history_lstm = lstm_model.fit(X_train, y_train,\n",
        "                              validation_data=(X_val, y_val),\n",
        "                              epochs=epochs_lstm,\n",
        "                              batch_size=batch_size,\n",
        "                              callbacks=[early_stop, reduce_lr],\n",
        "                              verbose=1)\n",
        "\n",
        "print(\"Training BiLSTM Model...\")\n",
        "history_bilstm = bilstm_model.fit(X_train, y_train,\n",
        "                                  validation_data=(X_val, y_val),\n",
        "                                  epochs=epochs_bilstm,\n",
        "                                  batch_size=batch_size,\n",
        "                                  callbacks=[early_stop, reduce_lr],\n",
        "                                  verbose=1)\n",
        "\n",
        "\n",
        "def evaluate_model(model, X, y, name):\n",
        "    loss, acc = model.evaluate(X, y, verbose=0)\n",
        "    print(f\"{name} Model Accuracy on test data: {acc:.4f}\")\n",
        "\n",
        "print(\"\\nEvaluating models on test data:\")\n",
        "evaluate_model(cnn_model, X_test, y_test, \"CNN\")\n",
        "evaluate_model(lstm_model, X_test, y_test, \"LSTM\")\n",
        "evaluate_model(bilstm_model, X_test, y_test, \"BiLSTM\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNdikhMeQdHs",
        "outputId": "11040e53-8535-4a16-cdb4-04da19fdd561"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training CNN Model...\n",
            "Epoch 1/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 13ms/step - accuracy: 0.3305 - loss: 1.7905 - val_accuracy: 0.3400 - val_loss: 1.1232 - learning_rate: 2.0000e-04\n",
            "Epoch 2/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3878 - loss: 1.5052 - val_accuracy: 0.3400 - val_loss: 1.1156 - learning_rate: 2.0000e-04\n",
            "Epoch 3/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4350 - loss: 1.3354 - val_accuracy: 0.3400 - val_loss: 1.0720 - learning_rate: 2.0000e-04\n",
            "Epoch 4/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4882 - loss: 1.1513 - val_accuracy: 0.4100 - val_loss: 0.9866 - learning_rate: 2.0000e-04\n",
            "Epoch 5/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5072 - loss: 1.0413 - val_accuracy: 0.6200 - val_loss: 0.8758 - learning_rate: 2.0000e-04\n",
            "Epoch 6/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5554 - loss: 1.0045 - val_accuracy: 0.8825 - val_loss: 0.7391 - learning_rate: 2.0000e-04\n",
            "Epoch 7/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5755 - loss: 0.8875 - val_accuracy: 0.9900 - val_loss: 0.6038 - learning_rate: 2.0000e-04\n",
            "Epoch 8/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6047 - loss: 0.8518 - val_accuracy: 0.9937 - val_loss: 0.5073 - learning_rate: 2.0000e-04\n",
            "Epoch 9/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6658 - loss: 0.7693 - val_accuracy: 0.9975 - val_loss: 0.4441 - learning_rate: 2.0000e-04\n",
            "Epoch 10/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6685 - loss: 0.7221 - val_accuracy: 1.0000 - val_loss: 0.3989 - learning_rate: 2.0000e-04\n",
            "Restoring model weights from the end of the best epoch: 10.\n",
            "Training LSTM Model...\n",
            "Epoch 1/12\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 247ms/step - accuracy: 0.3481 - loss: 1.7180 - val_accuracy: 0.4663 - val_loss: 1.2751 - learning_rate: 2.0000e-04\n",
            "Epoch 2/12\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 249ms/step - accuracy: 0.3558 - loss: 1.6210 - val_accuracy: 0.5437 - val_loss: 1.2522 - learning_rate: 2.0000e-04\n",
            "Epoch 3/12\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 233ms/step - accuracy: 0.3630 - loss: 1.4880 - val_accuracy: 0.5562 - val_loss: 1.2223 - learning_rate: 2.0000e-04\n",
            "Epoch 4/12\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 240ms/step - accuracy: 0.3559 - loss: 1.4438 - val_accuracy: 0.5750 - val_loss: 1.1877 - learning_rate: 2.0000e-04\n",
            "Epoch 5/12\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 250ms/step - accuracy: 0.3593 - loss: 1.4314 - val_accuracy: 0.5975 - val_loss: 1.1605 - learning_rate: 2.0000e-04\n",
            "Epoch 6/12\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 247ms/step - accuracy: 0.3753 - loss: 1.3736 - val_accuracy: 0.5500 - val_loss: 1.1548 - learning_rate: 2.0000e-04\n",
            "Epoch 7/12\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 233ms/step - accuracy: 0.3962 - loss: 1.3077 - val_accuracy: 0.5587 - val_loss: 1.1378 - learning_rate: 2.0000e-04\n",
            "Epoch 8/12\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 236ms/step - accuracy: 0.3928 - loss: 1.2956 - val_accuracy: 0.6075 - val_loss: 1.1127 - learning_rate: 2.0000e-04\n",
            "Epoch 9/12\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 250ms/step - accuracy: 0.3990 - loss: 1.2820 - val_accuracy: 0.6187 - val_loss: 1.1081 - learning_rate: 2.0000e-04\n",
            "Epoch 10/12\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 248ms/step - accuracy: 0.4164 - loss: 1.2619 - val_accuracy: 0.6575 - val_loss: 1.0847 - learning_rate: 2.0000e-04\n",
            "Epoch 11/12\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 249ms/step - accuracy: 0.4261 - loss: 1.2356 - val_accuracy: 0.6650 - val_loss: 1.0546 - learning_rate: 2.0000e-04\n",
            "Epoch 12/12\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 252ms/step - accuracy: 0.4695 - loss: 1.2003 - val_accuracy: 0.7613 - val_loss: 1.0023 - learning_rate: 2.0000e-04\n",
            "Restoring model weights from the end of the best epoch: 12.\n",
            "Training BiLSTM Model...\n",
            "Epoch 1/15\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 433ms/step - accuracy: 0.3302 - loss: 2.0759 - val_accuracy: 0.3650 - val_loss: 1.4940 - learning_rate: 2.0000e-04\n",
            "Epoch 2/15\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 424ms/step - accuracy: 0.3318 - loss: 1.9273 - val_accuracy: 0.3862 - val_loss: 1.4818 - learning_rate: 2.0000e-04\n",
            "Epoch 3/15\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 439ms/step - accuracy: 0.3346 - loss: 1.8689 - val_accuracy: 0.4313 - val_loss: 1.4599 - learning_rate: 2.0000e-04\n",
            "Epoch 4/15\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 441ms/step - accuracy: 0.3533 - loss: 1.7537 - val_accuracy: 0.4512 - val_loss: 1.4316 - learning_rate: 2.0000e-04\n",
            "Epoch 5/15\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 450ms/step - accuracy: 0.3405 - loss: 1.7063 - val_accuracy: 0.5038 - val_loss: 1.3948 - learning_rate: 2.0000e-04\n",
            "Epoch 6/15\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 442ms/step - accuracy: 0.3748 - loss: 1.6573 - val_accuracy: 0.5375 - val_loss: 1.3679 - learning_rate: 2.0000e-04\n",
            "Epoch 7/15\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 440ms/step - accuracy: 0.3667 - loss: 1.5993 - val_accuracy: 0.5688 - val_loss: 1.3514 - learning_rate: 2.0000e-04\n",
            "Epoch 8/15\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 440ms/step - accuracy: 0.3759 - loss: 1.5772 - val_accuracy: 0.6200 - val_loss: 1.3272 - learning_rate: 2.0000e-04\n",
            "Epoch 9/15\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 412ms/step - accuracy: 0.3958 - loss: 1.5279 - val_accuracy: 0.6787 - val_loss: 1.3043 - learning_rate: 2.0000e-04\n",
            "Epoch 10/15\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 432ms/step - accuracy: 0.4046 - loss: 1.4851 - val_accuracy: 0.7588 - val_loss: 1.2774 - learning_rate: 2.0000e-04\n",
            "Epoch 11/15\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 448ms/step - accuracy: 0.4022 - loss: 1.4828 - val_accuracy: 0.8050 - val_loss: 1.2419 - learning_rate: 2.0000e-04\n",
            "Epoch 12/15\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 426ms/step - accuracy: 0.4007 - loss: 1.4603 - val_accuracy: 0.8225 - val_loss: 1.2217 - learning_rate: 2.0000e-04\n",
            "Epoch 13/15\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 441ms/step - accuracy: 0.4506 - loss: 1.3983 - val_accuracy: 0.8725 - val_loss: 1.1778 - learning_rate: 2.0000e-04\n",
            "Epoch 14/15\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 442ms/step - accuracy: 0.4445 - loss: 1.3938 - val_accuracy: 0.9075 - val_loss: 1.1231 - learning_rate: 2.0000e-04\n",
            "Epoch 15/15\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 439ms/step - accuracy: 0.4777 - loss: 1.3405 - val_accuracy: 0.9075 - val_loss: 1.0845 - learning_rate: 2.0000e-04\n",
            "Restoring model weights from the end of the best epoch: 15.\n",
            "\n",
            "Evaluating models on test data:\n",
            "CNN Model Accuracy on test data: 1.0000\n",
            "LSTM Model Accuracy on test data: 0.7570\n",
            "BiLSTM Model Accuracy on test data: 0.9040\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SAVING THE BEST MODEL**"
      ],
      "metadata": {
        "id": "3aCjUFejE2zF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cnn_model.save('best_cnn_model.h5')\n",
        "print(\"Best CNN model saved to best_cnn_model.h5\")\n"
      ],
      "metadata": {
        "id": "8WW5Dhz2FWjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  predict sentiment for new text data\n",
        "def predict_sentiment(text, model, tokenizer, max_length=100):\n",
        "    # Preprocess the text using your existing function\n",
        "    processed_text = preprocess_text(text)\n",
        "\n",
        "    # Convert text to sequence and pad it\n",
        "    sequence = tokenizer.texts_to_sequences([processed_text])\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=max_length)\n",
        "\n",
        "\n",
        "    prediction = model.predict(padded_sequence)\n",
        "    predicted_class = np.argmax(prediction, axis=1)[0]\n",
        "\n",
        "    #mapping numeric prediction back to sentiment label\n",
        "    sentiment_mapping = {0: 'Frustrated', 1: 'Neutral', 2: 'Satisfied'}\n",
        "    predicted_sentiment = sentiment_mapping[predicted_class]\n",
        "\n",
        "    return predicted_sentiment, prediction\n",
        "\n",
        ":\n",
        "new_text = \"I am really happy with your service, everything was excellent.\"\n",
        "sentiment, probabilities = predict_sentiment(new_text, cnn_model, tokenizer, max_length)\n",
        "print(\"Predicted sentiment:\", sentiment)\n",
        "print(\"Prediction probabilities:\", probabilities)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nP8el1lW-HTz",
        "outputId": "478f4413-8555-4b6e-cd4f-877d5d99d92d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Predicted sentiment: Satisfied\n",
            "Prediction probabilities: [[0.24302487 0.2500682  0.50690687]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}